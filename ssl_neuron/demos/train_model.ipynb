{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "visible-inside",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "continental-prescription",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ssl_neuron.datasets import build_dataloader\n",
    "from ssl_neuron.graphdino import create_model\n",
    "from ssl_neuron.train import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-passion",
   "metadata": {},
   "source": [
    "#### Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "critical-syndrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open(\"../configs/config.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-stone",
   "metadata": {},
   "source": [
    "#### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "covered-civilian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphDINO(\n",
       "  (student_encoder): GraphTransformer(\n",
       "    (blocks): Sequential(\n",
       "      (0): AttentionBlock(\n",
       "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GraphAttention(\n",
       "          (qkv_projection): Linear(in_features=32, out_features=768, bias=False)\n",
       "          (proj): Linear(in_features=256, out_features=32, bias=True)\n",
       "          (predict_gamma): Linear(in_features=32, out_features=2, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (net): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=64, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GraphAttention(\n",
       "          (qkv_projection): Linear(in_features=32, out_features=768, bias=False)\n",
       "          (proj): Linear(in_features=256, out_features=32, bias=True)\n",
       "          (predict_gamma): Linear(in_features=32, out_features=2, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (net): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=64, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): AttentionBlock(\n",
       "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GraphAttention(\n",
       "          (qkv_projection): Linear(in_features=32, out_features=768, bias=False)\n",
       "          (proj): Linear(in_features=256, out_features=32, bias=True)\n",
       "          (predict_gamma): Linear(in_features=32, out_features=2, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (net): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=64, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): AttentionBlock(\n",
       "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GraphAttention(\n",
       "          (qkv_projection): Linear(in_features=32, out_features=768, bias=False)\n",
       "          (proj): Linear(in_features=256, out_features=32, bias=True)\n",
       "          (predict_gamma): Linear(in_features=32, out_features=2, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (net): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=64, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): AttentionBlock(\n",
       "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GraphAttention(\n",
       "          (qkv_projection): Linear(in_features=32, out_features=768, bias=False)\n",
       "          (proj): Linear(in_features=256, out_features=32, bias=True)\n",
       "          (predict_gamma): Linear(in_features=32, out_features=2, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (net): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=64, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): AttentionBlock(\n",
       "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GraphAttention(\n",
       "          (qkv_projection): Linear(in_features=32, out_features=768, bias=False)\n",
       "          (proj): Linear(in_features=256, out_features=32, bias=True)\n",
       "          (predict_gamma): Linear(in_features=32, out_features=2, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (net): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=64, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): AttentionBlock(\n",
       "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GraphAttention(\n",
       "          (qkv_projection): Linear(in_features=32, out_features=768, bias=False)\n",
       "          (proj): Linear(in_features=256, out_features=32, bias=True)\n",
       "          (predict_gamma): Linear(in_features=32, out_features=2, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (net): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=64, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (to_pos_embedding): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (mlp_head): Sequential(\n",
       "      (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Linear(in_features=32, out_features=32, bias=True)\n",
       "    )\n",
       "    (projector): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (3): GELU(approximate='none')\n",
       "      (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (7): Linear(in_features=128, out_features=1000, bias=True)\n",
       "    )\n",
       "    (to_node_embedding): Sequential(\n",
       "      (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (teacher_encoder): GraphTransformer(\n",
       "    (blocks): Sequential(\n",
       "      (0): AttentionBlock(\n",
       "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GraphAttention(\n",
       "          (qkv_projection): Linear(in_features=32, out_features=768, bias=False)\n",
       "          (proj): Linear(in_features=256, out_features=32, bias=True)\n",
       "          (predict_gamma): Linear(in_features=32, out_features=2, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (net): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=64, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GraphAttention(\n",
       "          (qkv_projection): Linear(in_features=32, out_features=768, bias=False)\n",
       "          (proj): Linear(in_features=256, out_features=32, bias=True)\n",
       "          (predict_gamma): Linear(in_features=32, out_features=2, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (net): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=64, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): AttentionBlock(\n",
       "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GraphAttention(\n",
       "          (qkv_projection): Linear(in_features=32, out_features=768, bias=False)\n",
       "          (proj): Linear(in_features=256, out_features=32, bias=True)\n",
       "          (predict_gamma): Linear(in_features=32, out_features=2, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (net): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=64, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): AttentionBlock(\n",
       "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GraphAttention(\n",
       "          (qkv_projection): Linear(in_features=32, out_features=768, bias=False)\n",
       "          (proj): Linear(in_features=256, out_features=32, bias=True)\n",
       "          (predict_gamma): Linear(in_features=32, out_features=2, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (net): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=64, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): AttentionBlock(\n",
       "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GraphAttention(\n",
       "          (qkv_projection): Linear(in_features=32, out_features=768, bias=False)\n",
       "          (proj): Linear(in_features=256, out_features=32, bias=True)\n",
       "          (predict_gamma): Linear(in_features=32, out_features=2, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (net): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=64, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): AttentionBlock(\n",
       "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GraphAttention(\n",
       "          (qkv_projection): Linear(in_features=32, out_features=768, bias=False)\n",
       "          (proj): Linear(in_features=256, out_features=32, bias=True)\n",
       "          (predict_gamma): Linear(in_features=32, out_features=2, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (net): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=64, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): AttentionBlock(\n",
       "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GraphAttention(\n",
       "          (qkv_projection): Linear(in_features=32, out_features=768, bias=False)\n",
       "          (proj): Linear(in_features=256, out_features=32, bias=True)\n",
       "          (predict_gamma): Linear(in_features=32, out_features=2, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (net): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=64, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (to_pos_embedding): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (mlp_head): Sequential(\n",
       "      (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Linear(in_features=32, out_features=32, bias=True)\n",
       "    )\n",
       "    (projector): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (3): GELU(approximate='none')\n",
       "      (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (7): Linear(in_features=128, out_features=1000, bias=True)\n",
       "    )\n",
       "    (to_node_embedding): Sequential(\n",
       "      (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model(config)\n",
    "model.train()\n",
    "# model.cuda();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-buffer",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "velvet-creation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 387/387 [00:03<00:00, 111.36it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 100.35it/s]\n"
     ]
    }
   ],
   "source": [
    "dataloaders = build_dataloader(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-tiffany",
   "metadata": {},
   "source": [
    "#### Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "corrected-seating",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(config, model, dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "focal-afghanistan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss 4.9886\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory ssl_neuron/ckpts does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/deep-neuronmorpho/notebooks/gnn_project_info/ssl_neuron/ssl_neuron/train.py:49\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_epoch(epoch)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# Save checkpoint.\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/repos/deep-neuronmorpho/notebooks/gnn_project_info/ssl_neuron/ssl_neuron/train.py:86\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     84\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mckpt_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch)\n\u001b[1;32m     85\u001b[0m PATH \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt_dir, filename)\n\u001b[0;32m---> 86\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSave model after epoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m as \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, filename))\n",
      "File \u001b[0;32m~/repos/deep-neuronmorpho/.venv/lib/python3.11/site-packages/torch/serialization.py:628\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    625\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 628\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    629\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/deep-neuronmorpho/.venv/lib/python3.11/site-packages/torch/serialization.py:502\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/deep-neuronmorpho/.venv/lib/python3.11/site-packages/torch/serialization.py:473\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 473\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory ssl_neuron/ckpts does not exist."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-melissa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
